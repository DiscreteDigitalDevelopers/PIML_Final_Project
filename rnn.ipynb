{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Movie Review Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as du\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2 \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Movie_Review_Data(Dataset):\n",
    "    '''\n",
    "    data_path: location of dataset\n",
    "    seq_len: maximum length of a sentence\n",
    "    embeddings_size: length of a word embedding vector\n",
    "    '''\n",
    "    def __init__(self, data_path, seq_len, embeddings, embeddings_size):\n",
    "        super(Movie_Review_Data, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.embeddings = embeddings\n",
    "        self.embeddings_size = embeddings_size\n",
    "        data_dict = None\n",
    "        with open(data_path, 'rb') as handle:\n",
    "            data_dict = pickle.load(handle)\n",
    "        if(data_dict is None):\n",
    "            return \"Invalid data path\"\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        for d in data_dict.items():\n",
    "            self.data.append(d[1:][0][1])\n",
    "            # self.labels.append(np.round(d[1:][0][0])-1)\n",
    "            self.labels.append(1 if d[1:][0][0] > 7 else 0)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "        # self.x = torch.ones(self.data.shape[0], seq_len, embeddings_size) * embeddings[\"_\"]  # the embedding of \"_\" is used as a padding token\n",
    "        # self.y = torch.tensor(self.labels)\n",
    "        # for i, d in enumerate(self.data):\n",
    "        #     for j, word in enumerate(d.split()):\n",
    "        #         try:\n",
    "        #             embed = embeddings[(word.lower()).translate(str.maketrans('', '', string.punctuation))]\n",
    "        #             self.x[i,j] = embed\n",
    "        #         except:\n",
    "        #             word = word.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "        #             for w in word.split():\n",
    "        #                 try:\n",
    "        #                     embed = embeddings[(w.lower()).translate(str.maketrans('', '', string.punctuation))]\n",
    "        #                     self.x[i,j] = embed\n",
    "        ##              except:\n",
    "        ##                  print(f\"Unable to parse word: {word}\")\n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        '''return len of dataset'''\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        '''return sequence, future sequence'''\n",
    "        # return self.x[idx], self.y[idx]\n",
    "        self.x = torch.ones(self.seq_len, self.embeddings_size) * self.embeddings[\"_\"]  # the embedding of \"_\" is used as a padding token\n",
    "        for j, word in enumerate(self.data[idx].split()):\n",
    "            if (j >= self.seq_len):\n",
    "                break\n",
    "            try:\n",
    "                embed = self.embeddings[(word.lower()).translate(str.maketrans('', '', string.punctuation))]\n",
    "            except:\n",
    "                word = word.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "                for w in word.split():\n",
    "                    try:\n",
    "                        embed = self.embeddings[(w.lower()).translate(str.maketrans('', '', string.punctuation))]\n",
    "                    except:\n",
    "                        embed = self.embeddings[\"_\"]\n",
    "                    else:\n",
    "                        self.x[j] = torch.tensor(embed)\n",
    "            else:\n",
    "                self.x[j] = torch.tensor(embed)\n",
    "        return self.x, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, in_feat, hidden_dim, num_layers, out_dim, dropout, seq_len):\n",
    "        '''\n",
    "        in_dim: input layer dim\n",
    "        hidden_layers: hidden layers in lstm\n",
    "        '''\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(in_feat, hidden_dim, num_layers, bidirectional=True, dropout=dropout, batch_first=True)\n",
    "        self.flatten = nn.Flatten()\n",
    "        # self.fc1 = nn.Linear(hidden_dim*seq_len*2, (hidden_dim*seq_len*2)//2)\n",
    "        # self.fc2 = nn.Linear((hidden_dim*seq_len*2)//2, out_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x, _ = self.lstm(x)\n",
    "        # x = self.fc1(F.relu(self.flatten(x)))\n",
    "        # x = self.fc2(F.relu(x))\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        x = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        # x = self.fc1(F.relu(self.flatten(x)))\n",
    "        x = self.fc1(self.flatten(x))\n",
    "        x = self.fc2(F.relu(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(200, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using device: {device}\")\n",
    "batch_size = 4\n",
    "learning_rate = 1e-4\n",
    "max_epochs = 100\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "# out_dim = 10\n",
    "out_dim = 2\n",
    "seq_len = 2000\n",
    "embeddings_size = 200\n",
    "seed = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "model = LSTM(embeddings_size, hidden_dim, num_layers, out_dim, dropout, seq_len)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "embeddings = {}\n",
    "with open(f\"Embeddings/glove.twitter.27B/glove.twitter.27B.{embeddings_size}d.txt\", \"r\", encoding=\"utf-8\") as f: # parsing file and saving each word embedding in a hashmap as {word: embedding}\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        embeddings[values[0]] = np.array(values[1:], dtype=\"float32\")\n",
    "\n",
    "# load training data in batches\n",
    "SAVE_LOCATION = './data/'\n",
    "train_loader = du.DataLoader(dataset=Movie_Review_Data(f'{SAVE_LOCATION}processed/data_train.pkl', seq_len, embeddings, embeddings_size, ),\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=True)\n",
    "dev_loader = du.DataLoader(dataset=Movie_Review_Data(f'{SAVE_LOCATION}processed/data_dev.pkl', seq_len, embeddings, embeddings_size),\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=True)\n",
    "test_loader = du.DataLoader(dataset=Movie_Review_Data(f'{SAVE_LOCATION}processed/data_test.pkl', seq_len, embeddings, embeddings_size),\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=True)\n",
    "# send model over to device\n",
    "model = model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:06<00:00,  4.11it/s]\n",
      "100%|██████████| 250/250 [00:37<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, training_loss 0.1653024829812348, training_accuracy 0.5997500419616699, dev_loss 0.15155702260136605, dev_accuracy 0.706000030040741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:58<00:00,  3.71it/s]\n",
      "100%|██████████| 250/250 [00:39<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, training_loss 0.15947962354309858, training_accuracy 0.6472500562667847, dev_loss 0.15162844339013098, dev_accuracy 0.6950000524520874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:58<00:00,  3.71it/s]\n",
      "100%|██████████| 250/250 [00:40<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, training_loss 0.15166995834745467, training_accuracy 0.6842500567436218, dev_loss 0.1545852716565132, dev_accuracy 0.659000039100647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:55<00:00,  4.20it/s]\n",
      "100%|██████████| 250/250 [00:33<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, training_loss 0.15370827978104354, training_accuracy 0.6681250333786011, dev_loss 0.15093438228964806, dev_accuracy 0.7020000219345093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:34<00:00,  3.88it/s]\n",
      "100%|██████████| 250/250 [00:37<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, training_loss 0.151339020896703, training_accuracy 0.687250018119812, dev_loss 0.14966784504055977, dev_accuracy 0.7000000476837158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 709/2000 [02:43<04:57,  4.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lij47\\ComputerScience\\Projects-In-ML\\PIML_Final_Project\\rnn.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lij47/ComputerScience/Projects-In-ML/PIML_Final_Project/rnn.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lij47/ComputerScience/Projects-In-ML/PIML_Final_Project/rnn.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# getting predictions from model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lij47/ComputerScience/Projects-In-ML/PIML_Final_Project/rnn.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m pred \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lij47/ComputerScience/Projects-In-ML/PIML_Final_Project/rnn.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# # calculating BCE loss between predictions and labels\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lij47/ComputerScience/Projects-In-ML/PIML_Final_Project/rnn.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(pred, target)\n",
      "File \u001b[1;32mc:\\Users\\lij47\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\lij47\\ComputerScience\\Projects-In-ML\\PIML_Final_Project\\rnn.ipynb Cell 6\u001b[0m in \u001b[0;36mLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lij47/ComputerScience/Projects-In-ML/PIML_Final_Project/rnn.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lij47/ComputerScience/Projects-In-ML/PIML_Final_Project/rnn.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# x, _ = self.lstm(x)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lij47/ComputerScience/Projects-In-ML/PIML_Final_Project/rnn.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# x = self.fc1(F.relu(self.flatten(x)))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lij47/ComputerScience/Projects-In-ML/PIML_Final_Project/rnn.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# x = self.fc2(F.relu(x))\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lij47/ComputerScience/Projects-In-ML/PIML_Final_Project/rnn.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     _, (hidden, _) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lij47/ComputerScience/Projects-In-ML/PIML_Final_Project/rnn.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((hidden[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m,:,:], hidden[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:,:]), dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lij47/ComputerScience/Projects-In-ML/PIML_Final_Project/rnn.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# x = self.fc1(F.relu(self.flatten(x)))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lij47\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\lij47\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:761\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    760\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 761\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    762\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    763\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    764\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    765\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "last_loss = np.inf\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "valid_acc_list = []\n",
    "# iterating through all epochs\n",
    "for epoch in range(1, max_epochs + 1):    \n",
    "    # training step\n",
    "    train_loss = 0.\n",
    "    train_accuracy = 0.\n",
    "    model.train()\n",
    "    # iterating through entire dataset in batches\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n",
    "        # sending batch over to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # zeroing out previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        # getting predictions from model\n",
    "        pred = model(data)\n",
    "        # # calculating BCE loss between predictions and labels\n",
    "        loss = F.cross_entropy(pred, target)\n",
    "        train_loss += loss.item()\n",
    "        # # calculating backprop and using an adam optimizer for update step \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_accuracy += torch.sum(torch.argmax(pred, dim=1) == target)\n",
    "\n",
    "    dev_loss = 0.\n",
    "    dev_accuracy = 0.\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # iterating through entire dataset in batches\n",
    "        for batch_idx, (data, target) in enumerate(tqdm(dev_loader)):\n",
    "            # sending batch over to device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # zeroing out previous gradients\n",
    "            optimizer.zero_grad()\n",
    "            # getting predictions from model\n",
    "            pred = model(data)\n",
    "            # # calculating BCE loss between predictions and labels\n",
    "            loss = F.cross_entropy(pred, target)\n",
    "            dev_loss += loss.item()\n",
    "            dev_accuracy += torch.sum(torch.argmax(pred, dim=1) == target)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_accuracy /= len(train_loader.dataset)\n",
    "    dev_loss /= len(dev_loader.dataset)\n",
    "    dev_accuracy /= len(dev_loader.dataset)\n",
    "    print(f\"Epoch: {epoch}, training_loss {train_loss}, training_accuracy {train_accuracy}, dev_loss {dev_loss}, dev_accuracy {dev_accuracy}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0be73a0e6356a5a1c38dc53f1e67790b18ed332d277068fed34d2daa61958429"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
